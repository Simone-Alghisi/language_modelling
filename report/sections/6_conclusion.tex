To summarise, the Baseline LSTM obtained $137$ PP on the test set of the Word-level PTB dataset, but showed high overfitting. Such behaviour was corrected by embedding several regularisation techniques in the proposed implementation of AWD-LSTM. This, together with the usage of NT-ASGD, and TBPTT with variable length led to an accuracy of $81.43$ PP. Finally, possible follow-up works should focus on better handling shorter sentences, being them the main cause of errors.
